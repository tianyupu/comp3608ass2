\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{natbib}
\pagestyle{fancy}
\lhead{SID: 310241316}
\rhead{SID: 310182212}

\title{COMP3608 Artificial Intelligence (Adv): \\
Email Classification}
\author{Dongjie Zhang: 310241316 
\\ Tianyu Pu: 310182212}
\date{\today}
\fancyhead[L]{310241316}
\fancyhead[R]{310182212}

\begin{document}
\maketitle

\section{Introduction}
\subsection{Aims}
To classify emails into one of two classes: spam and non-spam by implementing the 
Naive Bayes machine learning algorithm using relative frequencies of individual 
words appearing in the texts. The performance of the classifier is evaluated through 
10-fold cross validation and then compared with Weka's implementation of Naive Bayes 
and several other classification algorithms\footnote{http://cs.waikato.ac.nz/ml/weka/}.

\subsection{Why is this study important?}
The amount of digital information is growing at an increasing rate and it becomes 
increasingly difficult to search for information. Therefore, it is important to 
accurately classify the huge number of documents. To classify texts by creating 
rules specific to that field or text type requires expert knowledge would be costly 
and impractical. In comparison, the method explored here is completely general and 
can be used to classify texts from any area, not just emails into spam or non-spam 
(although fine-tuning of parameters will be necessary to maximise accuracy).

\section{Data Preprocessing}
\subsection{Procedure}
The LingSpam emails are split into two corpora: one containing the text of the 
subject and the other containing the text of the body. Individual words are extracted 
from each corpus, stopwords and punctuation are removed and document frequency of 
the words calculated. In each corpus the top 200 words by document frequency are 
selected as features and their values for each document is calculated using tf-idf 
weighting[footnote here explaining formula] and then normalised using cosine 
normalisation[footnote here].

\subsection{Results}
[number of words for each corpus before and after removing stop words]
\begin{tabular}{ | l | l | l | }
\hline
Corpus & \# Words before removing stopwords & \# Words after removing stopwords\\ \hline
Subject & 1234 & 123  \\ \hline
Body & 1234 & 123 \\
\hline
\end{tabular}
[top 100 words in each corpus and their df score]

[does the selection make sense given the task?]
[are the two lits similar? (DF scores for subject corpus lower because less text)]

\section{Results and Discussion}
Stratification: each class is represented with approximately equal proportions in both data sets (training and testing)
Cross validation: split data into 10 subsets, build classifier from 9 subsets, training on remaining subset.

\subsection{Findings}
\subsection{Discussion}
\subsubsection{Comparisons}
Should perform paired t-test here
\section{Extension}

\section{Conclusions and Next Steps}

\section{Reflection}

\bibliographystyle{plainnat}
\bibliography{bib}

\end{document}
