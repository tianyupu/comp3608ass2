\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{natbib}
\pagestyle{fancy}
\lhead{SID: 310241316}
\rhead{SID: 310182212}

\title{COMP3608 Artificial Intelligence (Adv): \\
Email Classification}
\author{Dongjie Zhang: 310241316 
\\ Tianyu Pu: 310182212}
\date{\today}
\fancyhead[L]{310241316}
\fancyhead[R]{310182212}

\begin{document}
\maketitle

\section{Introduction}
\subsection{Aims}
To classify emails into one of two classes: spam and non-spam by implementing the 
Naive Bayes machine learning algorithm using relative frequencies of individual 
words appearing in the texts. The performance of the classifier is evaluated through 
ten-fold cross validation and then compared with Weka's implementation of Naive Bayes 
and several other classification algorithms\footnote{http://cs.waikato.ac.nz/ml/weka/}.

\subsection{Why is this study important?}
The amount of digital information is growing at an increasing rate and it becomes 
increasingly difficult to search for information. Therefore, it is important to 
accurately classify the huge number of documents. To classify texts by creating 
rules specific to that field or text type requires expert knowledge would be costly 
and impractical. In comparison, the method explored here is completely general and 
can be used to classify texts from any area, not just emails into spam or non-spam 
(although fine-tuning of parameters will be necessary to maximise accuracy).

\section{Data Preprocessing}
\subsection{Procedure}
In order to be able to read the files, the Python \verb=gzip= module was used to
decompress the text format.
The LingSpam emails were split into two corpora: one containing the text of the 
subject and the other containing the text of the body. Individual words were extracted 
from each corpus, stopwords and punctuation removed and document frequency of 
the words calculated. In each corpus the top 200 words by document frequency were 
selected as features and then each feature was assigned a weighting using the
term frequency-inverse document frequency formula explained in \cite{sebastiani}.
These tf-idf scores are normalised to between 0 and 1 using the cosine normalisation
also described in \cite{sebastiani} and saved as two CSV files, one for each corpus.

\subsection{Preprocessing results}
The numer of words before and after stopword removal
\footnote{Stopword list courtesy of
http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-smart-stop-list/english.stop},
 are shown in the table below: \\\\
\begin{tabular}{ | l | l | l | }
\hline
Corpus & Words before stopword removal & Words after stopword removal\\ \hline
Subject & 1830 & 938  \\ \hline
Body & 153851 & 19929 \\ \hline
\end{tabular} \\\\
The top 100 words for each corpus, based on their DF (document frequency, or the number
of documents in the collection that the word appeared in) score, are shown below. \\\\
\begin{tabular}{ | l | l || l | l | }
\hline
\multicolumn{2}{| c ||}{Corpus: Subject}  & \multicolumn{2}{| c |}{Corpus: Body} \\ \hline
Word & Document Frequency & Word & Document Frequency\\ \hline
sum & 30 & information & 205\\ 
summary & 26 & language & 192\\ 
language & 21 & mail & 183\\ 
free & 20 & university & 179\\ 
disc & 19 & time & 178\\ 
english & 19 & list & 171\\ 
query & 18 & address & 165\\ 
linguistics & 15 & english & 159\\ 
sex & 13 & http & 156\\ 
words & 12 & linguistics & 156\\ 
opposites & 12 & people & 146\\ 
book & 10 & send & 146\\ 
method & 9 & free & 144\\ 
mail & 9 & make & 140\\ 
comparative & 9 & email & 133\\ 
correction & 8 & number & 128\\ 
ob & 8 & work & 128\\ 
program & 7 & www & 122\\ 
qs & 7 & languages & 119\\ 
million & 7 & find & 118\\ 
syntax & 7 & fax & 116\\ 
email & 7 & order & 108\\ 
announcement & 7 & call & 103\\ 
armey & 6 & form & 101\\ 
call & 6 & research & 100\\ 
slip & 6 & linguistic & 99\\ 
st & 6 & state & 99\\ 
internet & 6 & world & 98\\ 
japanese & 6 & years & 98\\ 
part & 6 & subject & 98\\ 
german & 6 & contact & 97\\ 
dick & 6 & de & 96\\ 
speaker & 6 & money & 94\\ 
workshop & 6 & word & 91\\ 
money & 6 & message & 91\\ 
business & 6 & ll & 89\\ 
lang & 6 & receive & 88\\ 
chinese & 5 & phone & 88\\ 
resources & 5 & check & 88\\ 
word & 5 & good & 87\\ 
list & 5 & interested & 86\\ 
languages & 5 & day & 86\\ 
native & 5 & year & 86\\ 
grammar & 5 & working & 85\\ 
research & 5 & case & 85\\ 
spanish & 5 & include & 85\\ 
needed & 5 & ve & 84\\ 
nglish & 5 & based & 84\\ 
linguist & 5 & note & 83\\ 
software & 5 & home & 83\\ 
hey & 5 & part & 83\\ 
time & 5 & made & 83\\ 
omparative & 4 & mailing & 81\\ 
address & 4 & including & 81\\ 
great & 4 & type & 80\\ 
fwd & 4 & web & 79\\ 
intuitions & 4 & give & 79\\ 
information & 4 & place & 79\\ 
banning & 4 & program & 79\\ 
american & 4 & date & 78\\ 
phonetics & 4 & line & 78\\ 
request & 4 & special & 78\\ 
web & 4 & days & 77\\ 
secrets & 4 & internet & 76\\ 
conference & 4 & back & 76\\ 
systems & 4 & service & 75\\ 
read & 4 & american & 75\\ 
programs & 4 & full & 74\\ 
summer & 4 & system & 74\\ 
www & 4 & business & 74\\ 
obs & 4 & ac & 73\\ 
pig & 4 & today & 73\\ 
synthetic & 3 & interest & 72\\ 
teaching & 3 & remove & 72\\ 
dutch & 3 & questions & 72\\ 
fall & 3 & john & 71\\ 
school & 3 & found & 70\\ 
linguists & 3 & related & 70\\ 
video & 3 & site & 69\\ 
french & 3 & linguist & 69\\ 
change & 3 & usa & 69\\ 
credit & 3 & read & 68\\ 
adjectives & 3 & point & 68\\ 
addresses & 3 & text & 68\\ 
names & 3 & ago & 67\\ 
live & 3 & week & 67\\ 
counting & 3 & book & 67\\ 
mac & 3 & cost & 66\\ 
youthese & 3 & dear & 66\\ 
policy & 3 & making & 66\\ 
decimal & 3 & simply & 65\\ 
dialect & 3 & question & 65\\ 
books & 3 & offer & 63\\ 
profit & 3 & received & 63\\ 
millions & 3 & general & 63\\ 
care & 3 & data & 62\\ 
misc & 3 & ca & 62\\ 
reference & 3 & important & 62\\ 
lists & 3 & summary & 61\\ 
released & 3 & long & 61\\
\end{tabular} \\\\

[does the selection make sense given the task?]
[are the two lits similar? (DF scores for subject corpus lower because less text)]

\section{Results and Discussion}
Stratification: each class is represented with approximately equal proportions in both data sets (training and testing)
Cross validation: split data into 10 subsets, build classifier from 9 subsets, training on remaining subset.

\subsection{Findings}
\begin{tabular}{ | l | l || l | l | }
\hline
\multicolumn{2}{| c ||}{Corpus: Subject}  & \multicolumn{2}{| c |}{Corpus: Body} \\ \hline
Classifier & Accuracy[\%] & Classifier & Accuracy[\%]\\ \hline
ZeroR & 12 & ZeroR & 34\\
OneR &  56 & OneR & 78\\
\hline
\end{tabular} \\\\

\subsection{Discussion}
\subsubsection{Comparisons}
Should perform paired t-test here
\section{Extension}

\section{Conclusions and Next Steps}

\section{Reflection}

\bibliographystyle{plainnat}
\bibliography{bib}

\end{document}
